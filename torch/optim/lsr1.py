# -*- coding: utf-8 -*-
"""LSR1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1d8sF52nLrkNV4iXqvKYSEZlvwzTzTWbX
"""

!pip3 install torchvision
!pip3 install scipy
!pip3 install functools
!pip3 install numpy as np
!pip3 install tqdm
!pip3 install torchvision

import torch
import scipy.optimize as sp
from functools import reduce
import math

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# This function was copied from torch.optim.LBFGS
def _cubic_interpolate(x1, f1, g1, x2, f2, g2, bounds=None):
    # ported from https://github.com/torch/optim/blob/master/polyinterp.lua
    # Compute bounds of interpolation area
    if bounds is not None:
        xmin_bound, xmax_bound = bounds
    else:
        xmin_bound, xmax_bound = (x1, x2) if x1 <= x2 else (x2, x1)

    # Code for most common case: cubic interpolation of 2 points
    #   w/ function and derivative values for both
    # Solution in this case (where x2 is the farthest point):
    #   d1 = g1 + g2 - 3*(f1-f2)/(x1-x2);
    #   d2 = sqrt(d1^2 - g1*g2);
    #   min_pos = x2 - (x2 - x1)*((g2 + d2 - d1)/(g2 - g1 + 2*d2));
    #   t_new = min(max(min_pos,xmin_bound),xmax_bound);
    d1 = g1 + g2 - 3 * (f1 - f2) / (x1 - x2)
    d2_square = d1**2 - g1 * g2
    if d2_square >= 0:
        d2 = d2_square.sqrt()
        if x1 <= x2:
            min_pos = x2 - (x2 - x1) * ((g2 + d2 - d1) / (g2 - g1 + 2 * d2))
        else:
            min_pos = x1 - (x1 - x2) * ((g1 + d2 - d1) / (g1 - g2 + 2 * d2))
        return min(max(min_pos, xmin_bound), xmax_bound)
    else:
        return (xmin_bound + xmax_bound) / 2.

# This function was copied from torch.optim.LBFGS
# This function use the strong wolfe conditions to get optimal step length
def _strong_wolfe(obj_func,
                  x,
                  t,
                  d,
                  f,
                  g,
                  gtd,
                  c1=1e-4,
                  c2=0.9,
                  tolerance_change=1e-9,
                  max_ls=25):
    # ported from https://github.com/torch/optim/blob/master/lswolfe.lua
    d_norm = d.abs().max()
    g = g.clone(memory_format=torch.contiguous_format)
    # evaluate objective and gradient using initial step
    f_new, g_new = obj_func(x, t, d)
    ls_func_evals = 1
    gtd_new = g_new.dot(d)

    # bracket an interval containing a point satisfying the Wolfe criteria
    t_prev, f_prev, g_prev, gtd_prev = 0, f, g, gtd
    done = False
    ls_iter = 0
    while ls_iter < max_ls:
        # check conditions
        if f_new > (f + c1 * t * gtd) or (ls_iter > 1 and f_new >= f_prev):
            bracket = [t_prev, t]
            bracket_f = [f_prev, f_new]
            bracket_g = [g_prev, g_new.clone(memory_format=torch.contiguous_format)]
            bracket_gtd = [gtd_prev, gtd_new]
            break

        if abs(gtd_new) <= -c2 * gtd:
            bracket = [t]
            bracket_f = [f_new]
            bracket_g = [g_new]
            done = True
            break

        if gtd_new >= 0:
            bracket = [t_prev, t]
            bracket_f = [f_prev, f_new]
            bracket_g = [g_prev, g_new.clone(memory_format=torch.contiguous_format)]
            bracket_gtd = [gtd_prev, gtd_new]
            break

        # interpolate
        min_step = t + 0.01 * (t - t_prev)
        max_step = t * 10
        tmp = t
        t = _cubic_interpolate(
            t_prev,
            f_prev,
            gtd_prev,
            t,
            f_new,
            gtd_new,
            bounds=(min_step, max_step))

        # next step
        t_prev = tmp
        f_prev = f_new
        g_prev = g_new.clone(memory_format=torch.contiguous_format)
        gtd_prev = gtd_new
        f_new, g_new = obj_func(x, t, d)
        ls_func_evals += 1
        gtd_new = g_new.dot(d)
        ls_iter += 1

    # reached max number of iterations?
    if ls_iter == max_ls:
        bracket = [0, t]
        bracket_f = [f, f_new]
        bracket_g = [g, g_new]

    # zoom phase: we now have a point satisfying the criteria, or
    # a bracket around it. We refine the bracket until we find the
    # exact point satisfying the criteria
    insuf_progress = False
    # find high and low points in bracket
    low_pos, high_pos = (0, 1) if bracket_f[0] <= bracket_f[-1] else (1, 0)
    while not done and ls_iter < max_ls:
        # line-search bracket is so small
        if abs(bracket[1] - bracket[0]) * d_norm < tolerance_change:
            break

        # compute new trial value
        t = _cubic_interpolate(bracket[0], bracket_f[0], bracket_gtd[0],
                               bracket[1], bracket_f[1], bracket_gtd[1])

        # test that we are making sufficient progress:
        # in case `t` is so close to boundary, we mark that we are making
        # insufficient progress, and if
        #   + we have made insufficient progress in the last step, or
        #   + `t` is at one of the boundary,
        # we will move `t` to a position which is `0.1 * len(bracket)`
        # away from the nearest boundary point.
        eps = 0.1 * (max(bracket) - min(bracket))
        if min(max(bracket) - t, t - min(bracket)) < eps:
            # interpolation close to boundary
            if insuf_progress or t >= max(bracket) or t <= min(bracket):
                # evaluate at 0.1 away from boundary
                if abs(t - max(bracket)) < abs(t - min(bracket)):
                    t = max(bracket) - eps
                else:
                    t = min(bracket) + eps
                insuf_progress = False
            else:
                insuf_progress = True
        else:
            insuf_progress = False

        # Evaluate new point
        f_new, g_new = obj_func(x, t, d)
        ls_func_evals += 1
        gtd_new = g_new.dot(d)
        ls_iter += 1

        if f_new > (f + c1 * t * gtd) or f_new >= bracket_f[low_pos]:
            # Armijo condition not satisfied or not lower than lowest point
            bracket[high_pos] = t
            bracket_f[high_pos] = f_new
            bracket_g[high_pos] = g_new.clone(memory_format=torch.contiguous_format)
            bracket_gtd[high_pos] = gtd_new
            low_pos, high_pos = (0, 1) if bracket_f[0] <= bracket_f[1] else (1, 0)
        else:
            if abs(gtd_new) <= -c2 * gtd:
                # Wolfe conditions satisfied
                done = True
            elif gtd_new * (bracket[high_pos] - bracket[low_pos]) >= 0:
                # old high becomes new low
                bracket[high_pos] = bracket[low_pos]
                bracket_f[high_pos] = bracket_f[low_pos]
                bracket_g[high_pos] = bracket_g[low_pos]
                bracket_gtd[high_pos] = bracket_gtd[low_pos]

            # new point becomes new low
            bracket[low_pos] = t
            bracket_f[low_pos] = f_new
            bracket_g[low_pos] = g_new.clone(memory_format=torch.contiguous_format)
            bracket_gtd[low_pos] = gtd_new

    # return stuff
    t = bracket[low_pos]
    f_new = bracket_f[low_pos]
    g_new = bracket_g[low_pos]
    return f_new, g_new, t, ls_func_evals


class LSR1(torch.optim.Optimizer):
    """
    .. Class of the the limited memory symmetric rank-1 update. 
        The first six functions are from torch.optim.LBFGS. 
        The step function has some parts which is from torch.optim.LBFGS.

    .. warning::
        This optimizer doesn't support per-parameter options and parameter
        groups (there can be only one).

    .. note::
        This is a very memory intensive optimizer. If it doesn't fit in memory
        try reducing the history size, or use a different algorithm.

    Args:
        lr (float): learning rate (default: 0.2)
        max_iter (int): maximal number of iterations per optimization step
            (default: 20)
        max_eval (int): maximal number of function evaluations per optimization
            step (default: max_iter * 1.25).
        tolerance_grad (float): termination tolerance on first order optimality
            (default: 1e-5).
        tolerance_change (float): termination tolerance on function
            value/parameter changes (default: 1e-9).
        tr_rho (float): initial radius of trust region (default = 0.7).
        gamma (float): scalar of the initial hess matrix (default = 1).
        c_1,c_2,c_3 (float): parameters for update radius (default: c_1=2. c_2=0, c_3=0.5)
        history_size (int): update history size (default: 3).
        line_search_fn (str): either 'strong_wolfe' or None (default: None).
        trust_method (str): either 'cauchy' or 'CG' (default: cauchy)
    """

    def __init__(self,
                 params,
                 lr=1,
                 max_iter=20,
                 tolerance_grad=1e-15,
                 tolerance_change=1e-15,
                 tr_radius = 0.7,
                 history_size=7,
                 mu_momentum = 0.5,
                 nu_momentum = 0.5,
                 alpha_S = 0.25,
                 newton_maxit = None,
                 cg_iter = None,
                 line_search_fn="strong_wolfe",
                 trust_method="Cauchy_Point_Calculation"):
        defaults = dict(
            lr=lr,
            max_iter=max_iter,
            tolerance_grad=tolerance_grad,
            tolerance_change=tolerance_change,
            tr_radius = tr_radius,
            history_size=history_size,
            mu_momentum = mu_momentum,
            nu_momentum = nu_momentum,
            alpha_S = alpha_S,
            newton_maxit = newton_maxit,
            cg_iter = cg_iter,
            line_search_fn=line_search_fn,
            trust_method=trust_method)
        super(LSR1, self).__init__(params, defaults)

        # From torch.optim.LBFGS 
        # it checks if is one dictionary with params
        if len(self.param_groups) != 1:
            raise ValueError("LSR1 doesn't support per-parameter options "
                             "(parameter groups)")

        # From torch.optim.LBFGS
        # unpack the paramaters
        self._params = self.param_groups[0]['params']
        self._numel_cache = None
        self.history_size = self.param_groups[0]['history_size']
        self.newton_maxit = self.param_groups[0]['newton_maxit']
        self.cg_iter = self.param_groups[0]['cg_iter']

    #From torch.optim.LBFGS
    # im not sure what happens
    # its needed for the flat_grad
    def _numel(self):
        if self._numel_cache is None:
            self._numel_cache = reduce(lambda total, p: total + p.numel(), self._params, 0)
        return self._numel_cache

    #From torch.optim.LBFGS
    # flat the gradient
    def _gather_flat_grad(self):
        views = []
        for p in self._params:
            if p.grad is None:
                view = p.new(p.numel()).zero_()
            elif p.grad.is_sparse:
                view = p.grad.to_dense().view(-1)
            else:
                view = p.grad.view(-1)
            views.append(view)
        return torch.cat(views, 0)

    #From torch.optim.LBFGS
    # do the update 
    def _add_grad(self, step_size, update):
        offset = 0
        for p in self._params:
            numel = p.numel()
            # view as to avoid deprecated pointwise semantics
            p.add_(update[offset:offset + numel].view_as(p), alpha=step_size)
            offset += numel
        assert offset == self._numel()

    #From torch.optim.LBFGS
    def _clone_param(self):
        return [p.clone(memory_format=torch.contiguous_format) for p in self._params]

    #From torch.optim.LBFGS
    def _set_param(self, params_data):
        for p, pdata in zip(self._params, params_data):
            p.copy_(pdata)

    #From torch.optim.LBFGS
    def _directional_evaluate(self, closure, x, t, d):
        self._add_grad(t, d)
        loss = float(closure())
        flat_grad = self._gather_flat_grad()
        self._set_param(x)
        return loss, flat_grad

    def trust_solver_None(self, M_inverse, P, lamb_gamma, tr_rho, gamma, flat_grad, psi):
      """
      .. The function solve a trust region subproblem. This was copied from https://github.com/MATHinDL/sL_QN_TR.
        Look in Subroutines in TRsubproblem_solver_OBS.m. Dont work very well.

      Args: 
          M_inverse (torch.tensor): a small matrix from the calculation of the hesse matrix
          P (torch.tensor) : The P from the hesse matrix
          tr_tho (float) : trust_radius
          gamma (float) : the gamma for the initial hess matrix
          flat_grad (torch.Tensor) : gradient vector
          psi (torch.tensor) : Y-B_0*S
          newton_maxit (int) : the max iterations for the newton method

      """
      def phi_phi_T(sigma, delta, a, lam):
        obs_tol = 1e-10
        t = lam + sigma
        if torch.sum(torch.abs(a) < obs_tol)> 0 or torch.sum(torch.abs(t) < obs_tol) > 0:
          llpll2 = 0
          llpll_T = 0
          for i in range(a):
            if torch.abs(a[i]) > obs_tol and torch.abs(t[i]) < obs_tol:
              return -1/delta, 1/obs_tol
            if torch.abs(a[i])> obs_tol and torch.abs(t[i]) > obs_tol:
              llpll2 = llpll2 + (a[i]/t[i])**2
              llpll_T = llpll_T + ( a[i]^2 /t[i]^3 )
          return 1/torch.sqrt(llpll2)- 1/delta
        llpll = torch.linalg.norm(a/t)
        return 1/llpll - 1/delta, llpll_T/(llpll^3)

      def equation_p1(psi, M_inverse, tau_star, flat_grad):
        psi_T = torch.tanspose(psi, 0, 1)
        Z = tau_star*M_inverse + torch.matmul(psi_T, psi)
        f = torch.matmul(psi_T, flat_grad)
        Zf = torch.linalg.solve(Z, f)
        return -(flat_grad - torch.matmul(psi, Zf))/tau_star

      def equation_p2(sigma, gamma, g, a, lam, P_l, g_l):
        t = lam+sigma
        c = len(t)
        v = torch.zeros(c).to(device)
        if torch.abs(gamma+ sigma < 1e-10):
          p = -torch.matmul(P, v[:-1])
        else:
          p = -torch.matmul(P, v[:-1]) - (g-torch.matmul(P, g_l))/(gamma+sigma)
        return p

      def equation_p3(lam_min, delta, p_hat,lam, P):
        alpha = torch.sqrt(delta**2 - torch.matmul(p_hat, p_hat))
        if torch.abs(lam_min - lam[0]) < 1e-10:
          u_min = P[:, 1]/torch.linalg.norm(P[:,1])
          z_star = alpha*u_min
        else:
          n, k = P.shape[0], P.shape[1]
          e = torch.zeros(n)
          found = 0
          for i in range(k):
            e[i] = 1
            u_min = e - torch.matmul(P, torch.transpose(P,0,1)[:,i])
            if torch.linalg.norm(u_min) > 1e-10:
              found = 1
              break
            e[i] = 0
          if found ==0:
            e[i+1] = 1
            u_min = e - torch.matmul(P, torch.transpose(P,0,1)[:,i+1])
          u_min = u_min/torch.linalg.norm(u_min)
          z_star = alpha * u_min
        return p_hat + z_star

      def newton_method(flat_grad, sigma_star, tr_rho, a, lam_all):
        newton_tol = 1e-10
        if self.newton_maxit == None:
          self.newton_maxit = flat_grad.shape[0]*self.history_size**2
        k = 0
        sigma_star = 0
        phi, phi_T = phi_phi_T(sigma_star, tr_rho, a, lam_all)
        while torch.abs(phi) > newton_tol and k < self.newton_maxit:
          sigma_star = sigma_star - phi/phi_T
          phi, phi_T = phi_phi_T(sigma_star, tr_rho, a, lam_all)
          k += 1
        return sigma_star + gamma
      
      lam_all = torch.cat((lamb_gamma, gamma), 0)
      lam_all = lam_all*( torch.abs(lam_all) > 1e-10 )
      lam_min = min(lam_all[0], gamma)
      g_ll = torch.matmul(torch.transpose(P, 0,1), flat_grad)
      gg = torch.matmul(flat_grad, flat_grad)
      gl_gl = torch.matmul(g_ll, g_ll)
      llg_perbll = torch.sqrt(torch.abs(gg-gl_gl))
      if llg_perbll^2 < 1e-10:
        llg_perbll = 0
      a = torch.cat((g_ll, llg_perbll), 0)
      if phi_phi_T(0, tr_rho, a, lam_all)[0] >= 0 and lam_min >0:
        sigma_star     = 0
        tau_star       = gamma + sigma_star
        psi_T = torch.tanspose(psi,0,1)
        Z = tau_star*M_inverse + torch.matmul(psi_T, psi)
        f = torch.matmul(psi_T, flat_grad)
        Zf = torch.linalg.solve(Z, f)
        p_star = -(flat_grad - torch.matmul(psi, Zf))/tau_star
      if lam_min <=0 and phi_phi_T(-lam_min, tr_rho, a, lam_all)>=0:
        sigma_star = -lam_min
        p_star = equation_p2(sigma_star, gamma, flat_grad, a, lam_all, P)
        if lam_min < 0:
          p_hat = p_star
          p_star = equation_p3(lam_min, tr_rho, p_hat, lam_all, P)
      else:
        if lam_min > 0:
          sigma_star = newton_method(flat_grad, 0, tr_rho, a, lam_all)
        else:
          sigma_hat = max(torch.abs(a)/tr_rho -lam_all)
          if sigma_hat > -lam_min:
            sigma_star = newton_method(flat_grad, sigma_hat, tr_rho, a, lam_all)
          else:
            sigma_star = newton_method(flat_grad, -lam_min, tr_rho, a, lam_all)
        tau_star = sigma_star + gamma
        p_star = equation_p1(psi, M_inverse, tau_star, flat_grad) 
      return p_star


    def calculate_M(self, S,Y,gamma):
      """
      .. Calculate the marix M = L + L^T + S^T*B_0*S + diag(S^T*Y) and return this.
        L is a lower left triangular matrix of S^TY.
        In Addition psi = Y-B_0*S return.

      Args:
          S (torch.tensor) : matrix which contains the old s as columns
          Y (torch.tensor) : matrix which contains the old y as columns
          gamma (float) : the scalar for the initial hess matrix 
      """
      dim_hess = S.shape[0]

      # B_{k} = B_0 + phi * M^{-1} * phi
      psi = Y - gamma*S

      #calculate M = D+L+L^T-S*B_0*S
      SY = torch.mm(torch.transpose(S, 0, 1), Y)
      SS = gamma*torch.mm(torch.transpose(S, 0, 1), S)
      L = torch.tril(SY, diagonal=-1)
      M = L + torch.transpose(L, 0,1) + SS
      mask_M = range(M.shape[0])
      M[mask_M, mask_M] = M[mask_M, mask_M] + torch.diag(SY)
      return M, psi

    # calculate hess with limited memory method
    def calculate_hess(self, psi, M_inverse, gamma):
      """
      .. Return P and lambdas+gamma. This are the components of the hess matrix.
        hess = P *diag(gamma+lambdas) * P^T. For this do a thin qr factorisation
        of psi. Use the M and R and to get RMR. Then do a spectral decomposition and 
        calculate Q*U = P. The eigenvalues of the composition are the lambdas.

      Args:
          S (torch.Tensor): saved s as a matrix, S.shape = (n, history_size)
          Y (torch.Tensor): saved y as a matrix, Y.shape = (n, history_size)
          gamma (float): skalar of initial hesse matrix
      """
      #thin q-r factorisation of phi
      Q, R = torch.linalg.qr(psi, mode="reduced")

      # eigenvalues and eigenvectors of RM^{-1}R^T
      
      RMR = torch.mm(torch.mm(R, M_inverse), torch.transpose(R,0,1))
      RMR = (RMR + torch.transpose(RMR,0,1))/2
      lamb, U = torch.linalg.eig(RMR)
                

      # create last orthogonal matrix QU
      P = torch.mm(Q, U.float())

      #create hess matrix, but dont calculate hess
      # save only the matrices hess_1 and hess_2 which hess = P_lamb @ P
      P_lamb = gamma+lamb.float()*P
      return P, gamma+lamb.float()

    def update_SY(self, s, y, old_s, old_y, cond_rest):
      """
      .. Update S and Y. Pop the first if history_size is reached.

      Args:
          s (torch.Tensor): actual s
          y (torch.Tensor): actual y
          old_s (list): list with tensors last s
          old_y (list): list with tensors last y
          cond_rest (float): one part of the condition to update S or Y

      """
      ys = y.dot(s)  # y*s
      if ys + cond_rest > 1e-10:
          # updating memory
          if len(old_s) == self.history_size:
              # shift history by one (limited-memory)
              old_s.pop(0)
              old_y.pop(0)

          # store new direction/step
          old_s.append(s)
          old_y.append(y)

    def update_radius(self, r, tr_rho, s, T, rho):
      """
      .. Update Radius like Algorithmus 5 in
        "A minibatch stochastic Quasi-Newton method adapted for nonconvex deep learning problems"
        from Joshua D. Griffin, Majid Jahani, Martin Takáč, Seyedalireza Yektamaram, Wenwen Zhou
        with a little change. The 0.1 in the first if is here 0.5.

      Args:
          r (float): ratio of actual and predicted reduction
          s (torch.tensor) : A vector w_k -w_{k-1}
          tr_rho (float): trust radius 
          T (float) : A iteration parameter, it is used to do the radius stochastic
          rho (float) : A iteration parameter, it is used to do the radius stochastic
      """
      rho = 0.5*T*rho + r
      T = 0.5*T + 1
      rho = rho/T
      norm_s = torch.linalg.norm(s)
      if rho < 0.5:
          tr_rho = min(tr_rho, norm_s)
      if rho >=  0.5 and norm_s >=tr_rho: 
          tr_rho = 2*tr_rho
      return tr_rho, rho, T

    def trust_solver_cauchy(self, flat_grad, hess_1, hess_2, tr_rho):
      """
      .. Solves a trust subproblem with the Cauchy Point Calculation. I copied this from
        https://optimization.mccormick.northwestern.edu/index.php/Trust-region_methods.

      Args: 
          flat_grad (torch.tensor) : gradient vector
          hess_1 (torch.tensor) : P*diag(gamma+lambda)*P^T = hess, hess_1 = P*diag(gamma+lambda)
          hess_2 (torch.tensor) : P*diag(gamma+lambda)*P^T = hess, hess_2 = P^T
          tr_rho (float) : the trust radius
      """
      gH = torch.matmul(flat_grad, hess_1)
      Hg = torch.matmul(hess_2, flat_grad)
      cauchy_cond = torch.matmul(gH, Hg)
      if cauchy_cond <=0:
          tau = 1
      else:
          tau = min(torch.linalg.norm(flat_grad)**3/(cauchy_cond*tr_rho),1)
      return -tau*tr_rho/torch.linalg.norm(flat_grad)*flat_grad

    def trust_solver_steihaug(self, flat_grad, hess_1, hess_2, tr_rho):
      """
      .. Solves a trust subproblem with the Steihaug CG Method. I copied this from
        https://optimization.mccormick.northwestern.edu/index.php/Trust-region_methods.
        The solution for tau can be calculated with the quadratic formula.
        The hyperparameters are from https://d-nb.info/1219852988/34. This download a pdf.
        I will search for better sources.

      Args: 
          flat_grad (torch.tensor) : gradient vector
          hess_1 (torch.tensor) : P*diag(gamma+lambda)*P^T = hess, hess_1 = P*diag(gamma+lambda)
          hess_2 (torch.tensor) : P*diag(gamma+lambda)*P^T = hess, hess_2 = P^T
          tr_rho (float) : the trust radius
      """
      n = flat_grad.shape[0]
      if self.cg_iter == None:
        self.cg_iter = n*(self.history_size**2)
      z = torch.zeros(n).to(device)
      r = flat_grad
      d = -r
      g_norm = torch.linalg.norm(flat_grad)
      delta = min(0.5, torch.sqrt(g_norm))*g_norm
      fH = torch.matmul(flat_grad, hess_1)
      Hf = torch.matmul(hess_2, flat_grad)
      if torch.matmul(fH, Hf) <= 0 :
        return -tr_rho/torch.linalg.norm(flat_grad)*flat_grad
      for _ in range(self.cg_iter):
          dH = torch.matmul(d, hess_1)
          Hd = torch.matmul(hess_2, d)
          dHd = torch.matmul(dH, Hd)
          dz = torch.matmul(d,z)
          dd = torch.matmul(d,d)
          zz = torch.matmul(z,z)
          if dHd <= 0:
            tau = (-2 * dz + torch.sqrt((-2*dz)**2 - 4* dd *(zz-tr_rho**2)))/(2*dd)
            return z + tau*d
          rr = torch.matmul(r,r)
          alpha = rr/dHd
          z_ersatz = z
          z = z+alpha*d
          if torch.linalg.norm(z) >= tr_rho:
            tau = (-2 * dz + torch.sqrt((-2*dz)**2 - 4* dd *(zz-tr_rho**2)))/(2*dd)
            return z_ersatz + tau*d
          r = r+ alpha*torch.matmul(hess_1, torch.matmul(hess_2, d))
          if torch.linalg.norm(r) < delta:
            return z
          beta = torch.matmul(r,r)/rr
          d = -r+beta*dd
      return z



    @torch.no_grad()
    def step(self, closure):
        """Performs a single optimization step.

        Args:
            closure (callable): A closure that reevaluates the model
                and returns the loss.
        """
        assert len(self.param_groups) == 1

        #From torch.optim.LBFGS
        # Make sure the closure is always called with grad enabled
        closure = torch.enable_grad()(closure)

        #load hyperparameter and settings
        group = self.param_groups[0]
        lr = group['lr']
        max_iter = group['max_iter']
        newton_maxit = group['newton_maxit']
        tolerance_grad = group['tolerance_grad']
        tolerance_change = group['tolerance_change']
        line_search_fn = group['line_search_fn']
        trust_method = group['trust_method']
        tr_radius = group['tr_radius']
        mu_momentum = group['mu_momentum']
        nu_momentum = group['nu_momentum']
        alpha_S = group['alpha_S']

        #From torch.optim.LBFGS
        # NOTE: LSR1 has only global state, but we register it as state for
        # the first param, because this helps with casting in load_state_dict
        state = self.state[self._params[0]]
        state.setdefault('n_iter', 0)

        # get loss
        # evaluate initial f(x) and df/dx
        #From torch.optim.LBFGS
        orig_loss = closure()
        loss = float(orig_loss)
        flat_grad = self._gather_flat_grad()
        opt_cond = flat_grad.abs().max() <= tolerance_grad

        #From torch.optim.LBFGS
        # optimal condition
        if opt_cond:
            return orig_loss

        # tensors cached in state (for tracing)
        d = state.get('d')
        v = state.get('v')
        alpha = state.get('alpha')
        old_s = state.get('old_s')
        old_y = state.get('old_y')
        prev_flat_grad = state.get('prev_flat_grad')
        prev_loss = state.get('prev_loss')
        tr_rho = state.get('tr_rho')
        s = state.get('s')
        y = state.get('y')
        T = state.get('T')
        rho = state.get('rho')

        #dimension of the data
        dim_hess = flat_grad.shape[0]

        n_iter = 0
        is_forever = 0
        # optimize for a max of max_iter iterations
        while n_iter < max_iter:
            # keep track of iterations
            n_iter += 1
            state['n_iter'] += 1

            ############################################################
            ####       compute gradient descent direction           ####
            ############################################################
            if state['n_iter'] == 1:
                # the first direction is the normal gradient
                # initialize parameters of the first step
                d = flat_grad.neg()
                old_s = []
                old_y = []
                hess_2 = torch.ones(1).to(device)
                hess_1 = torch.ones(1).to(device)
                gamma = 1
                tr_rho = tr_radius
                v = torch.zeros(dim_hess).to(device)
                s = torch.zeros(dim_hess).to(device)
                y = torch.zeros(dim_hess).to(device)
                T = 0
                rho=0
            else:
                if torch.linalg.norm(flat_grad) < tolerance_grad:
                    state['n_iter'] = 0
                    break
                
                # stack the list to a tensor 
                S = torch.transpose(torch.stack(old_s), 0, 1)
                Y = torch.transpose(torch.stack(old_y), 0, 1)

                #calculate gamma like in Stabilizied Barzilai-Borwein Method 
                #from Oleg Burdakov, Yu-Hong Dai, Na Huang
                gamma = torch.matmul(old_s[-1], old_s[-1])/(torch.matmul(old_s[-1], old_y[-1]))

                # calculate M and psi
                M, psi = self.calculate_M(S,Y, gamma)

                #check singular
                if torch.det(M)==0:
                  state['n_iter'] = 0
                  break
                
                #calculate the inverse of M
                M_inverse = torch.linalg.solve(M, torch.eye(M.shape[0]).to(device))

                #calculate the components of the hessian matrix
                P, lamb_gamma = self.calculate_hess(psi, M_inverse, gamma)
                hess_1 = lamb_gamma*P
                hess_2 = torch.transpose(P, 0, 1)

                #get the new search direction with Trust Region
                if trust_method == "Cauchy_Point_Calculation":
                  d = self.trust_solver_cauchy(flat_grad, hess_1, hess_2, tr_rho)
                if trust_method == "Steihaug_cg":
                  d = self.trust_solver_steihaug(flat_grad, hess_1, hess_2, tr_rho)
                if trust_method=="None":
                  d = self.trust_solver_None(M_inverse, P, lamb_gamma, tr_rho, flat_grad, psi, newton_maxit)

            # do some other options: momentum etc.
            v = mu_momentum*v - nu_momentum*alpha_S*flat_grad+(1-nu_momentum)*s
            v = min(1, tr_rho/torch.linalg.norm(v))*v
            d = (1-nu_momentum)*d + mu_momentum*v
            d = min(1, tr_rho/torch.linalg.norm(d))*d
            d = d.to(device)
            dg = abs(torch.matmul(d, flat_grad))

            # check if the serch direction is too orthogonal to gradient
            d_norm = abs(torch.linalg.norm(d))
            g_norm = abs(torch.linalg.norm(flat_grad))
            if  min(dg, dg/d_norm) < g_norm*5e-10:

              # the for_ever is useful to avoid enless loops 
              if is_forever == 1:
                old_s = []
                old_y = []
                s = None
                y = None
                v = None
                tr_rho = tr_radius
                state['n_iter'] = 0
                break
              n_iter=0
              state['n_iter'] = 0
              is_forever += 1
              continue

            #We need this for the actual prediction
            norm_g = torch.linalg.norm(flat_grad)
            if len(hess_1.shape) !=1:
              dH = torch.matmul(d, hess_1)
              Hd = torch.matmul(hess_2, d)
              dHd = torch.matmul(dH, Hd)
            else:
              dHd = gamma*torch.matmul(d, d)

            #############################################################
            #######     set lr, s, previous loss and flat_grad  #########
            #############################################################
            #From torch.optim.LBFGS
            # directional derivative
            gtd = flat_grad.dot(d)  # g * d
            # check descent direction
            if gtd > 0:
               d = -d

            #set s/alpha and a part of update condition/alpha
            s = torch.clone(d).to(device)
            if len(hess_1.shape) !=1:
              sH = torch.matmul(s, hess_1)
              Hs = torch.matmul(hess_2, s)
              cond_rest = torch.matmul(sH, Hs)
            else:
              cond_rest = gamma*torch.matmul(s, s)

            #From torch.optim.LBFGS
            # reset initial guess for step size
            if state['n_iter'] == 1:
                alpha = min(1., 1. / flat_grad.abs().sum()) * lr
            else:
                alpha = lr

            # update prev_loss
            prev_loss_t = loss

            #update prev_flat_grad
            #From torch.optim.LBFGS
            if prev_flat_grad is None:
                prev_flat_grad = flat_grad.clone(memory_format=torch.contiguous_format)
            else:
                prev_flat_grad.copy_(flat_grad)

            #############################################################
            #######               gradient step                 #########
            #############################################################
            #From torch.optim.LBFGS
            # optional line search: user function
            if line_search_fn is not None:
                # perform line search, using user function
                if line_search_fn != "strong_wolfe":
                    raise RuntimeError("only 'strong_wolfe' is supported")
                else:
                    x_init = self._clone_param()

                    def obj_func(x, t, d):
                        return self._directional_evaluate(closure, x, t, d)
                    loss_t, flat_grad_t, alpha_t, _ = _strong_wolfe(
                        obj_func, x_init, alpha, d, loss, flat_grad, gtd)
                # sometimes the search direction is so bad, that alpha can be zero
                # or very big. This produces nan in the loss
                # Avoid this and break
                if alpha_t != 0 and alpha_t < 5:
                    alpha = alpha_t
                    loss = loss_t
                    flat_grad = flat_grad_t
                    self._add_grad(alpha, d)
                    opt_cond = flat_grad.abs().max() <= tolerance_grad
                else:
                    old_s = []
                    old_y = []
                    s = None
                    y = None
                    v = None
                    tr_rho = tr_radius
                    state['n_iter'] = 0
                    break
            else:
                # no line search, simply move with fixed-step
                self._add_grad(alpha, d)
                if n_iter != max_iter:
                    with torch.enable_grad():
                        loss = float(closure())
                    flat_grad = self._gather_flat_grad()
                    opt_cond = flat_grad.abs().max() <= tolerance_grad
                    ls_func_evals = 1
            #calculate y
            y = flat_grad.sub(prev_flat_grad)

            #now we know alpha so we can use it
            s = alpha*s
            cond_rest = alpha*cond_rest

            #update prev_loss
            prev_loss = prev_loss_t

            #############################################################
            #######               update S,Y                    #########
            #############################################################
            self.update_SY(s, y, old_s, old_y, cond_rest)
            if len(old_s)==0:
                s = None
                y = None
                v = None
                tr_rho = tr_radius
                state['n_iter'] = 0
                break
            
            #############################################################
            #######               calculate ratio               #########
            #############################################################  
            ared = loss - prev_loss
            pred = loss + torch.matmul(flat_grad, d) + dHd
            r = ared/pred

            #############################################################
            #######               update radius                 #########
            #############################################################
            tr_rho, rho, T = self.update_radius(r, tr_rho, s, T, rho)

            ############################################################
            #####               check conditions                  ######
            ############################################################
            #From torch.optim.LBFGS
            if n_iter == max_iter:
                break

            # optimal condition, gradient is not zero
            if opt_cond:
                old_s = []
                old_y = []
                s = None
                y = None
                v = None
                tr_rho = tr_radius
                state['n_iter'] = 0
                break

            #From torch.optim.LBFGS
            # lack of progress
            if d.mul(alpha).abs().max() <= tolerance_change:
                old_s = []
                old_y = []
                s = None
                y = None
                v = None
                tr_rho = tr_radius
                state['n_iter'] = 0
                break

            #From torch.optim.LBFGS
            if abs(loss - prev_loss) < tolerance_change:
                old_s = []
                old_y = []
                s = None
                y = None
                v = None
                tr_rho = tr_radius
                state['n_iter'] = 0
                break

        state['d'] = d
        state['alpha'] = alpha
        state['old_s'] = old_s
        state['old_y'] = old_y
        state['prev_flat_grad'] = flat_grad
        state['prev_loss'] = loss
        state['tr_rho'] = tr_rho
        state['v'] = v
        state['s'] = s
        state['y'] = y
        state['T'] = T
        state['rho'] = rho

        return orig_loss

"""**Evaluate LSR1 on the rosenbrock function:**"""

import numpy as np
from tqdm import tqdm
def rosenbrock(xy):
    """
    ..Evaluate Rosenbrock function. Copied from
      https://github.com/jankrepl/mildlyoverfitted/blob/master/mini_tutorials/custom_optimizer_in_pytorch/src.py.
      It has some changes for my usecase.
    Parameters
    ----------
    xy : tuple
        Two element tuple of floats representing the x resp. y coordinates.
    Returns
    -------
    float
        The Rosenbrock function evaluated at the point `xy`.
    """
    x, y = xy

    return (1 - x) ** 2 + 100 * (y - x ** 2) ** 2

def run_optimization(xy_init, n_iter, optimizer):
    """Run optimization finding the minimum of the Rosenbrock function.
    Parameters
    ----------
    xy_init : tuple
        Two floats representing the x resp. y coordinates.
    optimizer_class : object
        Optimizer class.
    n_iter : int
        Number of iterations to run the optimization for.
    optimizer : An torch optimizer.
    Returns
    -------
    path : np.ndarray
        2D array of shape `(n_iter + 1, 2)`. Where the rows represent the
        iteration and the columns represent the x resp. y coordinates.
    """
    
    path = np.empty((n_iter + 1, 2))
    path[0, :] = xy_init.cpu().detach().numpy()

    for i in tqdm(range(1, n_iter + 1)):
        optimizer.zero_grad()
        loss = rosenbrock(xy_t)
        loss.backward()
        torch.nn.utils.clip_grad_norm_(xy_t, 1.0)
        def closure():
          if torch.is_grad_enabled():
            optimizer.zero_grad()
          loss = rosenbrock(xy_t)
          if loss.requires_grad:
            loss.backward()
          return loss
        optimizer.step(closure)

        path[i, :] = xy_t.cpu().detach().numpy()

    return path

#settings and startpoint
n_iter = 100
xy_init = torch.zeros(2).to(device)
xy_t = torch.tensor(xy_init, requires_grad=True).to(device)
trust_method="Steihaug_cg"

#set up optimizer and run, the last point ist path[-1]
optimizer = LSR1([xy_t], history_size=2, trust_method=trust_method, line_search_fn="strong_wolfe", max_iter=100)
path = run_optimization(xy_init, n_iter, optimizer)
path[-1]

"""**Now evaluate on a neuronalnetwork:** \\
First load data MNist and create network CNN: It contains one input layer, one hidden layer and linear layer at the end. 
"""

from torchvision import datasets as dts
from torchvision.transforms import ToTensor
traindt = dts.MNIST(
    root = 'data',
    train = True,                         
    transform = ToTensor(), 
    download = True,            
)
testdt = dts.MNIST(
    root = 'data', 
    train = False, 
    transform = ToTensor()
)

import torch.nn as nn
class CNN(nn.Module):
    def __init__(self):
        super(CNN, self).__init__()        
        self.conv1 = nn.Sequential(         
            nn.Conv2d(
                in_channels=1,              
                out_channels=3,            
                kernel_size=5,              
                stride=1,                   
                padding=2,                  
            ),                              
            nn.ReLU(),                      
            nn.MaxPool2d(kernel_size=2),    
        )
        self.conv2 = nn.Sequential(         
            nn.Conv2d(3, 3, 5, 1, 2),     
            nn.ReLU(),                      
            nn.MaxPool2d(2),
            #nn.BatchNorm2d(3)                
        )        # fully connected layer, output 10 classes
        self.out = nn.Linear(3 * 7 * 7, 10)    
        
    def forward(self, x):
        x = self.conv1(x)
        x = self.conv2(x)        # flatten the output of conv2 to (batch_size, 32 * 7 * 7)
        x = x.view(x.size(0), -1)       
        output = self.out(x)
        return output, x    # return x for visualization

"""define train function"""

from torch.autograd import Variable

loss_func = nn.CrossEntropyLoss()  
 
train_losses, test_losses = [], []
steps = 0
running_loss = 0
print_every = 10

loaders = {
    'train' : torch.utils.data.DataLoader(traindt, 
                                          batch_size=1024, 
                                          shuffle=True, 
                                          num_workers=1),
    
    'test'  : torch.utils.data.DataLoader(testdt, 
                                          batch_size=1024, 
                                          shuffle=True, 
                                          num_workers=1),
}

n = len(loaders['train'])
m = len(loaders['test'])
print(n)
def train(num_epochs, cnn, loaders, optimizer):
    running_loss = 0
    cnn.train()
        
    # Train the model
    total_step = len(loaders['train'])
        
    for epoch in range(num_epochs):
        for i, (images, labels) in enumerate(loaders['train']):
            images, labels = images.to(device), labels.to(device)

            # gives batch data, normalize x when iterate train_loader
            b_x = Variable(images)   # batch x
            b_y = Variable(labels)   # batch y
            output = cnn(b_x)[0] 
            
           
            loss = loss_func(output, b_y)
            
            # clear gradients for this training step   
            optimizer.zero_grad()           
            
            # backpropagation, compute gradients 
            loss.backward(retain_graph=True)   
            def closure():
                if torch.is_grad_enabled():
                  optimizer.zero_grad()
                output = cnn(b_x)[0]
                loss = loss_func(output, b_y)
                if loss.requires_grad:
                  loss.backward()
                return loss
                      
            aa = optimizer.step(closure = closure)  
            if torch.isnan(aa)==True:
              print(i)
            running_loss += loss.item()
            optimizer.zero_grad()              
            
            if (i+1) % 10 == 0:
                test_loss = 0
                accuracy = 0
                cnn.eval()


                with torch.no_grad():
                    for inputs, labelss in loaders['test']:

                        inputs, labelss = inputs.to(device),  labelss.to(device)
                        b_xx = Variable(inputs)   # batch x
                        b_yy = Variable(labelss)   # batch y
                        logps = cnn(b_xx)[0]
                        batch_loss = loss_func(logps, b_yy)
                        test_loss += batch_loss.item()
                    
                        ps = torch.exp(logps)
                        top_p, top_class = ps.topk(1, dim=1)
                        equals =  top_class == labelss.view(*top_class.shape)
                        accuracy +=   torch.mean(equals.type(torch.FloatTensor)).item()
                train_losses.append(running_loss/n)
                test_losses.append(test_loss/m)                    
                print(f"Epoch, Steps {epoch+1}/{num_epochs}, {i}.. "
                      f"Train loss: {running_loss/print_every:.3f}.. "
                      f"Test loss: {test_loss/m:.3f}.. "
                      f"Test accuracy: {accuracy/m:.3f}")
                running_loss = 0

"""And now test with LBFGS, LSR1, SGD, Adam

"""

num_epochs = 10
CNN_1 = CNN().to(device)
print("LSR1:")
#hs = 5, mu = 0.5, tr_rho=0.5
optimizer_LSR1 = LSR1(CNN_1.parameters(),lr=1, line_search_fn="strong_wolfe", alpha_S=0.5, history_size=16, tr_radius=0.9, mu_momentum=0.95, nu_momentum=0.75, max_iter=150, trust_method="Steihaug_cg") 
train(num_epochs, CNN_1, loaders, optimizer_LSR1)

CNN_2 = CNN().to(device)
print("LBFGS:")
optimizer_LBFGS = torch.optim.LBFGS(CNN_2.parameters(), line_search_fn="strong_wolfe") 
train(num_epochs, CNN_2, loaders, optimizer_LBFGS)

CNN_3 = CNN().to(device)
print("SGD:")
optimizer_SGD = torch.optim.SGD(CNN_3.parameters(), lr = 0.01) 
#train(num_epochs, CNN_3, loaders, optimizer_SGD)
CNN_4 = CNN().to(device)
print("ADAM:")
optimizer_ADAM = torch.optim.Adam(CNN_4.parameters(), lr = 0.001) 
train(80, CNN_4, loaders, optimizer_ADAM)

"""**Try with Cifar10**"""

import torch
import torch.nn as nn
import torchvision
import torchvision.transforms as transforms


# Device configuration
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

# Hyper-parameters
num_epochs = 3
learning_rate = 0.001

# Image preprocessing modules
transform = transforms.Compose([
    transforms.Pad(4),
    transforms.RandomHorizontalFlip(),
    transforms.RandomCrop(32),
    transforms.ToTensor()])

# CIFAR-10 dataset
train_dataset = torchvision.datasets.CIFAR10(root='../../data/',
                                             train=True, 
                                             transform=transform,
                                             download=True)

test_dataset = torchvision.datasets.CIFAR10(root='../../data/',
                                            train=False, 
                                            transform=transforms.ToTensor())

# Data loader
train_loader = torch.utils.data.DataLoader(dataset=train_dataset,
                                           batch_size=512, 
                                           shuffle=True)

test_loader = torch.utils.data.DataLoader(dataset=test_dataset,
                                          batch_size=512, 
                                          shuffle=False)
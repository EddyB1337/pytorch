{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import scipy.optimize as sp\n",
        "from functools import reduce\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# This function was copied from torch.optim.LBFGS\n",
        "def _cubic_interpolate(x1, f1, g1, x2, f2, g2, bounds=None):\n",
        "    # ported from https://github.com/torch/optim/blob/master/polyinterp.lua\n",
        "    # Compute bounds of interpolation area\n",
        "    if bounds is not None:\n",
        "        xmin_bound, xmax_bound = bounds\n",
        "    else:\n",
        "        xmin_bound, xmax_bound = (x1, x2) if x1 <= x2 else (x2, x1)\n",
        "\n",
        "    # Code for most common case: cubic interpolation of 2 points\n",
        "    #   w/ function and derivative values for both\n",
        "    # Solution in this case (where x2 is the farthest point):\n",
        "    #   d1 = g1 + g2 - 3*(f1-f2)/(x1-x2);\n",
        "    #   d2 = sqrt(d1^2 - g1*g2);\n",
        "    #   min_pos = x2 - (x2 - x1)*((g2 + d2 - d1)/(g2 - g1 + 2*d2));\n",
        "    #   t_new = min(max(min_pos,xmin_bound),xmax_bound);\n",
        "    d1 = g1 + g2 - 3 * (f1 - f2) / (x1 - x2)\n",
        "    d2_square = d1**2 - g1 * g2\n",
        "    if d2_square >= 0:\n",
        "        d2 = d2_square.sqrt()\n",
        "        if x1 <= x2:\n",
        "            min_pos = x2 - (x2 - x1) * ((g2 + d2 - d1) / (g2 - g1 + 2 * d2))\n",
        "        else:\n",
        "            min_pos = x1 - (x1 - x2) * ((g1 + d2 - d1) / (g1 - g2 + 2 * d2))\n",
        "        return min(max(min_pos, xmin_bound), xmax_bound)\n",
        "    else:\n",
        "        return (xmin_bound + xmax_bound) / 2.\n",
        "\n",
        "# This function was copied from torch.optim.LBFGS\n",
        "# This function use the strong wolfe conditions to get optimal step length\n",
        "def _strong_wolfe(obj_func,\n",
        "                  x,\n",
        "                  t,\n",
        "                  d,\n",
        "                  f,\n",
        "                  g,\n",
        "                  gtd,\n",
        "                  c1=1e-4,\n",
        "                  c2=0.9,\n",
        "                  tolerance_change=1e-9,\n",
        "                  max_ls=25):\n",
        "    # ported from https://github.com/torch/optim/blob/master/lswolfe.lua\n",
        "    d_norm = d.abs().max()\n",
        "    g = g.clone(memory_format=torch.contiguous_format)\n",
        "    # evaluate objective and gradient using initial step\n",
        "    f_new, g_new = obj_func(x, t, d)\n",
        "    ls_func_evals = 1\n",
        "    gtd_new = g_new.dot(d)\n",
        "\n",
        "    # bracket an interval containing a point satisfying the Wolfe criteria\n",
        "    t_prev, f_prev, g_prev, gtd_prev = 0, f, g, gtd\n",
        "    done = False\n",
        "    ls_iter = 0\n",
        "    while ls_iter < max_ls:\n",
        "        # check conditions\n",
        "        if f_new > (f + c1 * t * gtd) or (ls_iter > 1 and f_new >= f_prev):\n",
        "            bracket = [t_prev, t]\n",
        "            bracket_f = [f_prev, f_new]\n",
        "            bracket_g = [g_prev, g_new.clone(memory_format=torch.contiguous_format)]\n",
        "            bracket_gtd = [gtd_prev, gtd_new]\n",
        "            break\n",
        "\n",
        "        if abs(gtd_new) <= -c2 * gtd:\n",
        "            bracket = [t]\n",
        "            bracket_f = [f_new]\n",
        "            bracket_g = [g_new]\n",
        "            done = True\n",
        "            break\n",
        "\n",
        "        if gtd_new >= 0:\n",
        "            bracket = [t_prev, t]\n",
        "            bracket_f = [f_prev, f_new]\n",
        "            bracket_g = [g_prev, g_new.clone(memory_format=torch.contiguous_format)]\n",
        "            bracket_gtd = [gtd_prev, gtd_new]\n",
        "            break\n",
        "\n",
        "        # interpolate\n",
        "        min_step = t + 0.01 * (t - t_prev)\n",
        "        max_step = t * 10\n",
        "        tmp = t\n",
        "        t = _cubic_interpolate(\n",
        "            t_prev,\n",
        "            f_prev,\n",
        "            gtd_prev,\n",
        "            t,\n",
        "            f_new,\n",
        "            gtd_new,\n",
        "            bounds=(min_step, max_step))\n",
        "\n",
        "        # next step\n",
        "        t_prev = tmp\n",
        "        f_prev = f_new\n",
        "        g_prev = g_new.clone(memory_format=torch.contiguous_format)\n",
        "        gtd_prev = gtd_new\n",
        "        f_new, g_new = obj_func(x, t, d)\n",
        "        ls_func_evals += 1\n",
        "        gtd_new = g_new.dot(d)\n",
        "        ls_iter += 1\n",
        "\n",
        "    # reached max number of iterations?\n",
        "    if ls_iter == max_ls:\n",
        "        bracket = [0, t]\n",
        "        bracket_f = [f, f_new]\n",
        "        bracket_g = [g, g_new]\n",
        "\n",
        "    # zoom phase: we now have a point satisfying the criteria, or\n",
        "    # a bracket around it. We refine the bracket until we find the\n",
        "    # exact point satisfying the criteria\n",
        "    insuf_progress = False\n",
        "    # find high and low points in bracket\n",
        "    low_pos, high_pos = (0, 1) if bracket_f[0] <= bracket_f[-1] else (1, 0)\n",
        "    while not done and ls_iter < max_ls:\n",
        "        # line-search bracket is so small\n",
        "        if abs(bracket[1] - bracket[0]) * d_norm < tolerance_change:\n",
        "            break\n",
        "\n",
        "        # compute new trial value\n",
        "        t = _cubic_interpolate(bracket[0], bracket_f[0], bracket_gtd[0],\n",
        "                               bracket[1], bracket_f[1], bracket_gtd[1])\n",
        "\n",
        "        # test that we are making sufficient progress:\n",
        "        # in case `t` is so close to boundary, we mark that we are making\n",
        "        # insufficient progress, and if\n",
        "        #   + we have made insufficient progress in the last step, or\n",
        "        #   + `t` is at one of the boundary,\n",
        "        # we will move `t` to a position which is `0.1 * len(bracket)`\n",
        "        # away from the nearest boundary point.\n",
        "        eps = 0.1 * (max(bracket) - min(bracket))\n",
        "        if min(max(bracket) - t, t - min(bracket)) < eps:\n",
        "            # interpolation close to boundary\n",
        "            if insuf_progress or t >= max(bracket) or t <= min(bracket):\n",
        "                # evaluate at 0.1 away from boundary\n",
        "                if abs(t - max(bracket)) < abs(t - min(bracket)):\n",
        "                    t = max(bracket) - eps\n",
        "                else:\n",
        "                    t = min(bracket) + eps\n",
        "                insuf_progress = False\n",
        "            else:\n",
        "                insuf_progress = True\n",
        "        else:\n",
        "            insuf_progress = False\n",
        "\n",
        "        # Evaluate new point\n",
        "        f_new, g_new = obj_func(x, t, d)\n",
        "        ls_func_evals += 1\n",
        "        gtd_new = g_new.dot(d)\n",
        "        ls_iter += 1\n",
        "\n",
        "        if f_new > (f + c1 * t * gtd) or f_new >= bracket_f[low_pos]:\n",
        "            # Armijo condition not satisfied or not lower than lowest point\n",
        "            bracket[high_pos] = t\n",
        "            bracket_f[high_pos] = f_new\n",
        "            bracket_g[high_pos] = g_new.clone(memory_format=torch.contiguous_format)\n",
        "            bracket_gtd[high_pos] = gtd_new\n",
        "            low_pos, high_pos = (0, 1) if bracket_f[0] <= bracket_f[1] else (1, 0)\n",
        "        else:\n",
        "            if abs(gtd_new) <= -c2 * gtd:\n",
        "                # Wolfe conditions satisfied\n",
        "                done = True\n",
        "            elif gtd_new * (bracket[high_pos] - bracket[low_pos]) >= 0:\n",
        "                # old high becomes new low\n",
        "                bracket[high_pos] = bracket[low_pos]\n",
        "                bracket_f[high_pos] = bracket_f[low_pos]\n",
        "                bracket_g[high_pos] = bracket_g[low_pos]\n",
        "                bracket_gtd[high_pos] = bracket_gtd[low_pos]\n",
        "\n",
        "            # new point becomes new low\n",
        "            bracket[low_pos] = t\n",
        "            bracket_f[low_pos] = f_new\n",
        "            bracket_g[low_pos] = g_new.clone(memory_format=torch.contiguous_format)\n",
        "            bracket_gtd[low_pos] = gtd_new\n",
        "\n",
        "    # return stuff\n",
        "    t = bracket[low_pos]\n",
        "    f_new = bracket_f[low_pos]\n",
        "    g_new = bracket_g[low_pos]\n",
        "    return f_new, g_new, t, ls_func_evals\n",
        "\n",
        "\n",
        "class LSR1(torch.optim.Optimizer):\n",
        "    \"\"\"\n",
        "    .. Class of the the limited memory symmetric rank-1 update. \n",
        "        The first six functions are from torch.optim.LBFGS. \n",
        "        The step function has some parts which is from torch.optim.LBFGS.\n",
        "\n",
        "    .. warning::\n",
        "        This optimizer doesn't support per-parameter options and parameter\n",
        "        groups (there can be only one).\n",
        "\n",
        "    .. note::\n",
        "        This is a very memory intensive optimizer. If it doesn't fit in memory\n",
        "        try reducing the history size, or use a different algorithm.\n",
        "\n",
        "    Args:\n",
        "        lr (float): learning rate (default: 0.2)\n",
        "        max_iter (int): maximal number of iterations per optimization step\n",
        "            (default: 20)\n",
        "        max_eval (int): maximal number of function evaluations per optimization\n",
        "            step (default: max_iter * 1.25).\n",
        "        tolerance_grad (float): termination tolerance on first order optimality\n",
        "            (default: 1e-5).\n",
        "        tolerance_change (float): termination tolerance on function\n",
        "            value/parameter changes (default: 1e-9).\n",
        "        tr_rho (float): initial radius of trust region (default = 0.7).\n",
        "        gamma (float): scalar of the initial hess matrix (default = 1).\n",
        "        c_1,c_2,c_3 (float): parameters for update radius (default: c_1=2. c_2=0, c_3=0.5)\n",
        "        history_size (int): update history size (default: 3).\n",
        "        line_search_fn (str): either 'strong_wolfe' or None (default: None).\n",
        "        trust_method (str): either 'cauchy' or 'CG' (default: cauchy)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 params,\n",
        "                 lr=0.2,\n",
        "                 max_iter=20,\n",
        "                 max_eval=None,\n",
        "                 tolerance_grad=1e-15,\n",
        "                 tolerance_change=1e-9,\n",
        "                 tr_rho = 0.8,\n",
        "                 history_size=7,\n",
        "                 gamma=1,\n",
        "                 c_1 = 2,\n",
        "                 c_2 = 0,\n",
        "                 c_3 = 0.5,\n",
        "                 trust_CG_tol=1e-2,\n",
        "                 mu_momentum = 0.5,\n",
        "                 nu_momentum = 0.5,\n",
        "                 alpha_S = 0.5,\n",
        "                 trust_CG_iter=None,\n",
        "                 line_search_fn=\"strong_wolfe\",\n",
        "                 trust_method=None):\n",
        "        if max_eval is None:\n",
        "            max_eval = max_iter * 5 // 4\n",
        "        defaults = dict(\n",
        "            lr=lr,\n",
        "            max_iter=max_iter,\n",
        "            max_eval=max_eval,\n",
        "            tolerance_grad=tolerance_grad,\n",
        "            tolerance_change=tolerance_change,\n",
        "            tr_rho = tr_rho,\n",
        "            history_size=history_size,\n",
        "            gamma=gamma,\n",
        "            c_1 = c_1,\n",
        "            c_2 = c_2,\n",
        "            c_3 = c_3,\n",
        "            trust_CG_tol=trust_CG_tol,\n",
        "            mu_momentum = mu_momentum,\n",
        "            nu_momentum = nu_momentum,\n",
        "            alpha_S = alpha_S,\n",
        "            trust_CG_iter=trust_CG_iter,\n",
        "            line_search_fn=line_search_fn,\n",
        "            trust_method=trust_method)\n",
        "        super(LSR1, self).__init__(params, defaults)\n",
        "\n",
        "        # From torch.optim.LBFGS \n",
        "        # it checks if is one dictionary with params\n",
        "        if len(self.param_groups) != 1:\n",
        "            raise ValueError(\"LSR1 doesn't support per-parameter options \"\n",
        "                             \"(parameter groups)\")\n",
        "\n",
        "        # From torch.optim.LBFGS\n",
        "        # unpack the paramaters\n",
        "        self._params = self.param_groups[0]['params']\n",
        "        self._numel_cache = None\n",
        "        self.history_size = self.param_groups[0]['history_size']\n",
        "\n",
        "    #From torch.optim.LBFGS\n",
        "    # im not sure what happens\n",
        "    # its needed for the flat_grad\n",
        "    def _numel(self):\n",
        "        if self._numel_cache is None:\n",
        "            self._numel_cache = reduce(lambda total, p: total + p.numel(), self._params, 0)\n",
        "        return self._numel_cache\n",
        "\n",
        "    #From torch.optim.LBFGS\n",
        "    # flat the gradient\n",
        "    def _gather_flat_grad(self):\n",
        "        views = []\n",
        "        for p in self._params:\n",
        "            if p.grad is None:\n",
        "                view = p.new(p.numel()).zero_()\n",
        "            elif p.grad.is_sparse:\n",
        "                view = p.grad.to_dense().view(-1)\n",
        "            else:\n",
        "                view = p.grad.view(-1)\n",
        "            views.append(view)\n",
        "        return torch.cat(views, 0)\n",
        "\n",
        "    #From torch.optim.LBFGS\n",
        "    # do the update \n",
        "    def _add_grad(self, step_size, update):\n",
        "        offset = 0\n",
        "        for p in self._params:\n",
        "            numel = p.numel()\n",
        "            # view as to avoid deprecated pointwise semantics\n",
        "            p.add_(update[offset:offset + numel].view_as(p), alpha=step_size)\n",
        "            offset += numel\n",
        "        assert offset == self._numel()\n",
        "\n",
        "    #From torch.optim.LBFGS\n",
        "    def _clone_param(self):\n",
        "        return [p.clone(memory_format=torch.contiguous_format) for p in self._params]\n",
        "\n",
        "    #From torch.optim.LBFGS\n",
        "    def _set_param(self, params_data):\n",
        "        for p, pdata in zip(self._params, params_data):\n",
        "            p.copy_(pdata)\n",
        "\n",
        "    #From torch.optim.LBFGS\n",
        "    def _directional_evaluate(self, closure, x, t, d):\n",
        "        self._add_grad(t, d)\n",
        "        loss = float(closure())\n",
        "        flat_grad = self._gather_flat_grad()\n",
        "        self._set_param(x)\n",
        "        return loss, flat_grad\n",
        "\n",
        "    def solve_trust_sub(self,flat_grad, hess_1, hess_2, tr_rho, trust_method, trust_CG_tol, trust_CG_iter):\n",
        "      \"\"\"\n",
        "      .. The function solve a trust region subproblem.\n",
        "\n",
        "      Args: \n",
        "          flat_grad (torch.Tensor): gradient vector\n",
        "          hess (torch.Tensor): hesse matrix\n",
        "          tr_tho (float): trust_radius\n",
        "          trust_method (string): None, cauchy or CG. If None then cauchy\n",
        "          trust_CG_tol: toleranz for reiduum for CG Steighaug\n",
        "          trust:CG_iter: number of max iterations for CG Steighaug\n",
        "      \"\"\"\n",
        "      # cauchy_condition : g^T * Hessematrix * g\n",
        "      if trust_method ==None or trust_method==\"cauchy\":\n",
        "        gH = torch.matmul(flat_grad, hess_1)\n",
        "        Hg = torch.matmul(hess_2, flat_grad)\n",
        "        cauchy_cond = torch.matmul(gH, Hg)\n",
        "        if cauchy_cond <=0:\n",
        "            tau = 1\n",
        "        else:\n",
        "            tau = min(torch.linalg.norm(flat_grad)**3/(cauchy_cond*tr_rho),1)\n",
        "        return -tau*tr_rho/torch.linalg.norm(flat_grad)*flat_grad\n",
        "      #performs CG Steighaug method for trust problem\n",
        "      if trust_method == \"CG\":\n",
        "        func = lambda a: torch.matmul(flat_grad, a)+torch.matmul(torch.matmul(a, hess_1),torch.matmul(hess_2, a))\n",
        "        z = torch.ones(flat_grad.shape[0]).to(device)\n",
        "        rr = flat_grad\n",
        "        dd = -rr\n",
        "        tol = trust_CG_tol\n",
        "        if torch.linalg.norm(rr) < tol :\n",
        "          return z\n",
        "        for _ in range(trust_CG_iter):\n",
        "          dH = torch.matmul(dd, hess_1)\n",
        "          Hd = torch.matmul(hess_2, dd)\n",
        "          dHd = torch.matmul(dH, Hd)\n",
        "          dz = torch.matmul(dd,z)\n",
        "          ddd = torch.matmul(dd,dd)\n",
        "          zz = torch.matmul(z,z)\n",
        "          root = abs(-(zz-tr_rho**2)/ddd + (dz/ddd)**2)\n",
        "          no_root = dz/ddd\n",
        "          tau_1 = -no_root + torch.sqrt(root)\n",
        "          tau_2 = -no_root - torch.sqrt(root)\n",
        "          tau = 1\n",
        "          if dHd <= 0:\n",
        "            if func(z+tau_1*dd)> func(z+tau_2*dd) :\n",
        "              tau = tau_2\n",
        "            else:\n",
        "              tau = tau_1\n",
        "            return z+tau*dd\n",
        "          rrr = torch.matmul(rr,rr)\n",
        "          alpha2 = rrr/dHd\n",
        "          z_ersatz = z\n",
        "          z = z+alpha2*dd\n",
        "          if torch.linalg.norm(z) >= tr_rho:\n",
        "            if tau_1 >= 0:\n",
        "              tau = tau_1\n",
        "            else: \n",
        "              tau = tau_2\n",
        "            return z_ersatz+tau*dd\n",
        "          rr = rr+ alpha2*torch.matmul(hess_1, torch.matmul(hess_2, dd))\n",
        "          if torch.linalg.norm(rr) < tol:\n",
        "            return z\n",
        "          beta = torch.matmul(rr,rr)/rrr\n",
        "          dd = -rr+beta*dd\n",
        "\n",
        "    # calculate hess with limited memory method\n",
        "    def calculate_hess(self, S, Y, gamma):\n",
        "      \"\"\"\n",
        "      .. Calculate the hess matrix with the limited memory method.\n",
        "\n",
        "      Args:\n",
        "          S (torch.Tensor): saved s as a matrix, S.shape = (n, history_size)\n",
        "          Y (torch.Tensor): saved y as a matrix, Y.shape = (n, history_size)\n",
        "          gamma (float): skalar of initial hesse matrix\n",
        "      \"\"\"\n",
        "      dim_hess = S.shape[0]\n",
        "\n",
        "      # B_{k} = B_0 + phi * M^{-1} * phi\n",
        "      phi = Y - gamma*S\n",
        "\n",
        "      #calculate M = D+L+L^T-S*B_0*S\n",
        "      SY = torch.mm(torch.transpose(S, 0, 1), Y)\n",
        "      SS = gamma*torch.mm(torch.transpose(S, 0, 1), S)\n",
        "      L = torch.tril(SY, diagonal=-1)\n",
        "      M = L + torch.transpose(L, 0,1) + SS\n",
        "      mask_M = range(M.shape[0])\n",
        "      M[mask_M, mask_M] = M[mask_M, mask_M] + torch.diag(SY)\n",
        "\n",
        "      #delete unnecessary matrices\n",
        "      del Y\n",
        "      del S       \n",
        "      del SY\n",
        "      del L\n",
        "      del SS\n",
        "\n",
        "      #check M singular, if yes then go to the next step\n",
        "      if torch.det(M)==0:\n",
        "          return False, 1, 1\n",
        "\n",
        "      #calculate the inverse of M\n",
        "      M_inverse = torch.linalg.solve(M, torch.eye(M.shape[0]).to(device)) \n",
        "      del M\n",
        "\n",
        "      #thin q-r factorisation of phi\n",
        "      Q, R = torch.linalg.qr(phi, mode=\"reduced\")\n",
        "      del phi\n",
        "\n",
        "      # eigenvalues and eigenvectors of RM^{-1}R^T\n",
        "      RMR = torch.mm(torch.mm(R, M_inverse), torch.transpose(R,0,1))\n",
        "      lamb, U = torch.linalg.eig(RMR)\n",
        "                \n",
        "      del R\n",
        "      del M_inverse\n",
        "\n",
        "      # create last orthogonal matrix QU\n",
        "      P = torch.mm(Q, U.float())\n",
        "      del Q\n",
        "      del U\n",
        "\n",
        "      #create hess matrix, but dont calculate hess\n",
        "      # save only the matrices hess_1 and hess_2 which hess = hess_1 @ hess_2\n",
        "      P_1 = gamma+lamb.float()*P\n",
        "      ones_column = torch.ones(P.shape[0]).reshape(-1,1).to(device)\n",
        "      ones_row = torch.ones(P.shape[0]).reshape(1,-1).to(device)\n",
        "      hess_1 = torch.cat((P_1, gamma*ones_column), 1).to(device)\n",
        "      hess_2 = torch.cat((torch.transpose(P,0,1), ones_row),0).to(device)\n",
        "      hess_2[-1, range(lamb.shape[0])]\n",
        "      del P\n",
        "      return True, hess_1, hess_2\n",
        "\n",
        "    def update_SY(self, s, y, old_s, old_y, cond_rest):\n",
        "      \"\"\"\n",
        "      .. Update S and Y. Pop the first if history_size is reached.\n",
        "\n",
        "      Args:\n",
        "          s (torch.Tensor): actual gradient vector\n",
        "          y (torch.Tensor): previous gradient vector\n",
        "          old_s (list): list with tensors last s\n",
        "          old_y (list): list with tensors last y\n",
        "          hess (torch.Tensor): hess matrix\n",
        "          cond_rest (float): one part of the condition to update S or Y\n",
        "\n",
        "      \"\"\"\n",
        "      ys = y.dot(s)  # y*s\n",
        "      if ys + cond_rest > 1e-10:\n",
        "          # updating memory\n",
        "          if len(old_s) == self.history_size:\n",
        "              # shift history by one (limited-memory)\n",
        "              old_s.pop(0)\n",
        "              old_y.pop(0)\n",
        "\n",
        "          # store new direction/step\n",
        "          old_s.append(s)\n",
        "          old_y.append(y)\n",
        "\n",
        "    #bearbeiten!!!!!!!\n",
        "    def update_radius(self, r, c_1, c_2, c_3, tr_rho):\n",
        "      \"\"\"\n",
        "      Args:\n",
        "          r (float): ratio of actual and predicted reduction\n",
        "          c_1,c_2,c_2 (flaot): hyperparameters of the changing radius\n",
        "          tr_rho (float): trust radius \n",
        "      \"\"\"\n",
        "      if r <= c_2:\n",
        "          tr_rho = c_3*tr_rho\n",
        "      else : \n",
        "          tr_rho = c_1*tr_rho\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def step(self, closure):\n",
        "        \"\"\"Performs a single optimization step.\n",
        "\n",
        "        Args:\n",
        "            closure (callable): A closure that reevaluates the model\n",
        "                and returns the loss.\n",
        "        \"\"\"\n",
        "        assert len(self.param_groups) == 1\n",
        "\n",
        "        #From torch.optim.LBFGS\n",
        "        # Make sure the closure is always called with grad enabled\n",
        "        closure = torch.enable_grad()(closure)\n",
        "\n",
        "        #load hyperparameter and settings\n",
        "        group = self.param_groups[0]\n",
        "        lr = group['lr']\n",
        "        max_iter = group['max_iter']\n",
        "        max_eval = group['max_eval']\n",
        "        gamma = group['gamma']\n",
        "        tolerance_grad = group['tolerance_grad']\n",
        "        tolerance_change = group['tolerance_change']\n",
        "        line_search_fn = group['line_search_fn']\n",
        "        trust_method = group['trust_method']\n",
        "        tr_rho = group['tr_rho']\n",
        "        mu_momentum = group['mu_momentum']\n",
        "        nu_momentum = group['nu_momentum']\n",
        "        alpha_S = group['alpha_S']\n",
        "        c_1 = group['c_1']\n",
        "        c_2 = group['c_2']\n",
        "        c_3 = group['c_3']\n",
        "        trust_CG_tol = group['trust_CG_tol']\n",
        "        trust_CG_iter = group['trust_CG_iter']\n",
        "        \n",
        "        #From torch.optim.LBFGS\n",
        "        # NOTE: LSR1 has only global state, but we register it as state for\n",
        "        # the first param, because this helps with casting in load_state_dict\n",
        "        state = self.state[self._params[0]]\n",
        "        state.setdefault('func_evals', 0)\n",
        "        state.setdefault('n_iter', 0)\n",
        "\n",
        "        #From torch.optim.LBFGS\n",
        "        # evaluate initial f(x) and df/dx\n",
        "        orig_loss = closure()\n",
        "        loss = float(orig_loss)\n",
        "        current_evals = 1\n",
        "        state['func_evals'] += 1\n",
        "\n",
        "        #From torch.optim.LBFGS\n",
        "        #df/dx\n",
        "        flat_grad = self._gather_flat_grad()\n",
        "        opt_cond = flat_grad.abs().max() <= tolerance_grad\n",
        "\n",
        "        #From torch.optim.LBFGS\n",
        "        # optimal condition\n",
        "        if opt_cond:\n",
        "            return orig_loss\n",
        "\n",
        "        # tensors cached in state (for tracing)\n",
        "        d = state.get('d')\n",
        "        v = state.get('v')\n",
        "        alpha = state.get('alpha')\n",
        "        old_s = state.get('old_s')\n",
        "        old_y = state.get('old_y')\n",
        "        prev_flat_grad = state.get('prev_flat_grad')\n",
        "        prev_loss = state.get('prev_loss')\n",
        "        tr_radius = state.get('tr_rho')\n",
        "        s = state.get('s')\n",
        "\n",
        "        dim_hess = flat_grad.shape[0]\n",
        "\n",
        "        #check s is defined\n",
        "        if s == None:\n",
        "          s = torch.zeros(dim_hess)\n",
        "\n",
        "        #check initial radius\n",
        "        if tr_radius != None:\n",
        "          tr_rho = tr_radius\n",
        "\n",
        "        #check iterations of CG\n",
        "        \n",
        "        if trust_CG_iter == None:\n",
        "          trust_CG_iter = dim_hess\n",
        "\n",
        "        n_iter = 0\n",
        "        # optimize for a max of max_iter iterations\n",
        "        while n_iter < max_iter:\n",
        "            # keep track of iterations\n",
        "            n_iter += 1\n",
        "            state['n_iter'] += 1\n",
        "\n",
        "            ############################################################\n",
        "            ####       compute gradient descent direction           ####\n",
        "            ############################################################\n",
        "            if state['n_iter'] == 1:\n",
        "                # the first direction is the normal gradient\n",
        "                # initialize parameters of the first step\n",
        "                d = flat_grad.neg()\n",
        "                old_s = []\n",
        "                old_y = []\n",
        "                hess_1 = gamma*torch.eye(dim_hess).to(device)\n",
        "                hess_2 = torch.eye(dim_hess).to(device)\n",
        "                tr_rho = tr_rho\n",
        "                v = torch.zeros(dim_hess).to(device)\n",
        "            else:\n",
        "                # stack the list to a tensor \n",
        "                S = torch.transpose(torch.stack(old_s), 0, 1)\n",
        "                Y = torch.transpose(torch.stack(old_y), 0, 1)\n",
        "\n",
        "                # the approximate hess matrix\n",
        "                cond_M, hess_1, hess_2 = self.calculate_hess(S,Y, gamma)\n",
        "                #check if matrix \"M\" is singular\n",
        "                if cond_M == False:\n",
        "                  continue\n",
        "\n",
        "                #solve trust region subproblem\n",
        "                d = self.solve_trust_sub(flat_grad, hess_1, hess_2, tr_rho, trust_method, trust_CG_tol, trust_CG_iter)\n",
        "                # do some other options: momentum etc.\n",
        "                v = mu_momentum*v - nu_momentum*alpha_S*flat_grad+(1-nu_momentum)*s\n",
        "                v = min(1, tr_rho/torch.linalg.norm(v))*v\n",
        "                d = (1-nu_momentum)*d + mu_momentum*v\n",
        "                d = min(1, tr_rho/torch.linalg.norm(d))*d\n",
        "                d = d.to(device)\n",
        "            dH = torch.matmul(d, hess_1)\n",
        "            Hd = torch.matmul(hess_2, d)\n",
        "            dHd = torch.matmul(dH, Hd)\n",
        "\n",
        "            #############################################################\n",
        "            #######     set lr, s, previous loss and flat_grad  #########\n",
        "            #############################################################\n",
        "            #From torch.optim.LBFGS\n",
        "            # directional derivative\n",
        "            gtd = flat_grad.dot(d)  # g * d\n",
        "\n",
        "            # directional derivative is below tolerance\n",
        "            if gtd > 0:\n",
        "               d = -d\n",
        "\n",
        "            #set s/alpha and a part of update condition/alpha\n",
        "            # then we can delete hess\n",
        "            s = d\n",
        "            sH = torch.matmul(s, hess_1)\n",
        "            Hs = torch.matmul(hess_2, s)\n",
        "            cond_rest = torch.matmul(sH, Hs)\n",
        "            del hess_1\n",
        "            del hess_2\n",
        "\n",
        "            #From torch.optim.LBFGS\n",
        "            # reset initial guess for step size\n",
        "            if state['n_iter'] == 1:\n",
        "                alpha = min(1., 1. / flat_grad.abs().sum()) * lr\n",
        "            else:\n",
        "                alpha = lr\n",
        "\n",
        "            # update prev_loss\n",
        "            prev_loss = loss\n",
        "            #update prev_flat_grad\n",
        "            if prev_flat_grad is None:\n",
        "                prev_flat_grad = flat_grad.clone(memory_format=torch.contiguous_format)\n",
        "            else:\n",
        "                prev_flat_grad.copy_(flat_grad)\n",
        "\n",
        "            #############################################################\n",
        "            #######               gradient step                 #########\n",
        "            #############################################################\n",
        "            #From torch.optim.LBFGS\n",
        "            # optional line search: user function\n",
        "            ls_func_evals = 0\n",
        "            if line_search_fn is not None:\n",
        "                # perform line search, using user function\n",
        "                if line_search_fn != \"strong_wolfe\":\n",
        "                    raise RuntimeError(\"only 'strong_wolfe' is supported\")\n",
        "                else:\n",
        "                    x_init = self._clone_param()\n",
        "\n",
        "                    def obj_func(x, t, d):\n",
        "                        return self._directional_evaluate(closure, x, t, d)\n",
        "\n",
        "                    loss, flat_grad, alpha, ls_func_evals = _strong_wolfe(\n",
        "                        obj_func, x_init, alpha, d, loss, flat_grad, gtd)\n",
        "                self._add_grad(alpha, d)\n",
        "                opt_cond = flat_grad.abs().max() <= tolerance_grad\n",
        "            else:\n",
        "                # no line search, simply move with fixed-step\n",
        "                self._add_grad(alpha, d)\n",
        "                if n_iter != max_iter:\n",
        "                    with torch.enable_grad():\n",
        "                        loss = float(closure())\n",
        "                    flat_grad = self._gather_flat_grad()\n",
        "                    opt_cond = flat_grad.abs().max() <= tolerance_grad\n",
        "                    ls_func_evals = 1\n",
        "            y = flat_grad.sub(prev_flat_grad)\n",
        "            #now we know alpha so we can use it\n",
        "            s = alpha*s\n",
        "            cond_rest = alpha*cond_rest\n",
        "\n",
        "            #############################################################\n",
        "            #######               update S,Y                    #########\n",
        "            #############################################################\n",
        "            self.update_SY(s, y, old_s, old_y, cond_rest)\n",
        "\n",
        "            #############################################################\n",
        "            #######               calculate ratio               #########\n",
        "            #############################################################  \n",
        "            ared = loss - prev_loss\n",
        "            pred = loss + torch.matmul(flat_grad, d) + dHd\n",
        "            r = ared/pred\n",
        "\n",
        "            #############################################################\n",
        "            #######               update radius                 #########\n",
        "            #############################################################\n",
        "            if state['n_iter']!= 1:\n",
        "                self.update_radius(r, c_1, c_2, c_3, tr_rho)\n",
        "\n",
        "            #From torch.optim.LBFGS\n",
        "            # update func eval\n",
        "            current_evals += ls_func_evals\n",
        "            state['func_evals'] += ls_func_evals\n",
        "\n",
        "            ############################################################\n",
        "            #####               check conditions                  ######\n",
        "            ############################################################\n",
        "            #From torch.optim.LBFGS\n",
        "            if n_iter == max_iter:\n",
        "                break\n",
        "\n",
        "            #From torch.optim.LBFGS\n",
        "            if current_evals >= max_eval:\n",
        "                break\n",
        "\n",
        "            # optimal condition, gradient is not zero\n",
        "            if opt_cond:\n",
        "                break\n",
        "\n",
        "            #From torch.optim.LBFGS\n",
        "            # lack of progress\n",
        "            if d.mul(alpha).abs().max() <= tolerance_change:\n",
        "                break\n",
        "            #From torch.optim.LBFGS\n",
        "            if abs(loss - prev_loss) < tolerance_change:\n",
        "                break\n",
        "\n",
        "        state['d'] = d\n",
        "        state['alpha'] = alpha\n",
        "        state['old_s'] = old_s\n",
        "        state['old_y'] = old_y\n",
        "        state['prev_flat_grad'] = flat_grad\n",
        "        state['prev_loss'] = loss\n",
        "        state['tr_rho'] = tr_rho\n",
        "        state['v'] = v\n",
        "        state['s'] = s\n",
        "\n",
        "\n",
        "        return orig_loss"
      ],
      "metadata": {
        "id": "cS5iqujxMcLh"
      },
      "execution_count": 7,
      "outputs": []
    }
  ]
}